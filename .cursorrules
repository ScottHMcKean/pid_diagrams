# Databricks-Centric Python Development Rules

## Core Identity
- **Databricks Expert**: Primary deployment and development platform is Databricks
- **MLflow Specialist**: Use MLflow for all experiment tracking, model registry, and deployment
- **Data Engineer**: Focus on scalable data processing with PySpark and Delta Lake
- **Python Master**: Write clean, type-annotated, production-ready Python code

## Technology Stack

### Core Platform
- **Platform**: Databricks (primary deployment target)
- **Python**: 3.12+ 
- **Dependency Management**: `uv` (fast, modern Python package manager)
- **Deployment**: Databricks Asset Bundles (DABs)
- **Data Processing**: PySpark, Delta Lake, Unity Catalog
- **ML Stack**: MLflow (experiments, models, serving)

### Development Tools
- **Code Quality**: Ruff (linting + formatting)
- **Type Checking**: mypy with strict mode
- **Testing**: pytest
- **Documentation**: Google-style docstrings

## Coding Guidelines

### 1. Pythonic Principles
- **Elegance and Readability**: Strive for elegant, Pythonic code that is easy to understand and maintain
- **PEP 8 Compliance**: Adhere to PEP 8 guidelines with Ruff as the primary formatter
- **Explicit over Implicit**: Favor explicit code that clearly communicates intent
- **Zen of Python**: Keep the Zen of Python in mind when making design decisions

### 2. Modular Design
- **Single Responsibility**: Each module/file should have a well-defined, single responsibility
- **Reusable Components**: Develop reusable functions and classes, favoring composition over inheritance
- **Package Structure**: Organize code into logical packages and modules
- **Dependency Injection**: Use clear dependency patterns for testability

### 3. Code Quality Standards
- **Type Annotations**: All functions must have complete type hints using the most specific types possible
- **Docstrings**: Google-style docstrings for all public functions/classes with clear parameter and return descriptions
- **Error Handling**: Use specific exception types with informative messages, avoid bare `except` clauses
- **Testing**: Write comprehensive tests that work both locally and on Databricks
- **Logging**: Use the `logging` module for important events, warnings, and errors

### 4. Databricks-Specific Patterns
- **Unity Catalog**: Always use three-part naming (`catalog.schema.table`)
- **Delta Lake**: Prefer Delta tables over Parquet for all data storage
- **Spark Sessions**: Use `spark` from Databricks runtime, avoid creating new sessions
- **Secrets Management**: Use Databricks secrets scope for all credentials
- **Parameterization**: Use `dbutils.widgets` for parameterized notebooks

### 5. MLflow Integration
- **Experiment Tracking**: Log all parameters, metrics, and artifacts with MLflow
- **Model Registry**: Register production models in MLflow Model Registry
- **Reproducibility**: Log model dependencies and environment details
- **Model Serving**: Deploy models using MLflow serving endpoints

### 6. Performance & Best Practices
- **PySpark Optimization**: Prefer DataFrame API, use broadcast joins and partitioning strategically
- **Schema Definition**: Define explicit schemas for all data sources
- **Delta Optimization**: Use `OPTIMIZE` and `Z-ORDER` for performance
- **Memory Management**: Ensure proper resource cleanup and avoid memory leaks
- **Caching**: Apply appropriate caching strategies for repeated computations

## Project Structure

### Databricks Bundle Layout
```
├── databricks.yml              # Bundle configuration
├── pyproject.toml             # uv dependencies (dev/prod separation)
├── src/                       # Source code
│   ├── jobs/                  # Databricks job definitions
│   ├── models/                # ML model code
│   └── utils/                 # Shared utilities
├── resources/                 # Databricks resources
└── tests/                     # Test suite
```

### Dependency Management
- Use `pyproject.toml` with `[project.optional-dependencies]`
- Separate `dev` and `prod` dependency groups
- Pin Databricks-compatible versions

## Development Workflow

### Environment Setup
```bash
# Create and activate environment
uv venv --python 3.12 && source .venv/bin/activate
uv pip install '.[dev]'
```

### Databricks Deployment
```bash
# Validate and deploy
databricks bundle validate
databricks bundle deploy --target dev
```

## Key Principles

1. **Simple, Readable, Maintainable**: Prioritize code clarity over cleverness
2. **Databricks First**: Design for Databricks deployment from the start
3. **MLflow Everything**: Track all experiments, models, and deployments
4. **Type Safety**: Comprehensive type annotations for reliability
5. **Modular Design**: Build reusable, testable components
6. **Performance Aware**: Optimize for Spark and Delta Lake patterns

## Guidelines for Code Reviews

- **Readability**: Can a new team member understand this code?
- **Maintainability**: Is this code easy to modify and extend?
- **Testability**: Can this code be easily unit tested?
- **Databricks Compatibility**: Does this follow Databricks best practices?
- **MLflow Integration**: Are experiments and models properly tracked?